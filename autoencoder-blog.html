<!DOCTYPE html>
 <html lang="en">
  <head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width,initial-scale=1.0"> <title>Understanding Autoencoders | Soumya Savarn</title> 
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
  <script src="https://cdn.jsdelivr.net/npm/tensorflow@2.8.0/dist/tf.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
  <style> :root { --primary-color: #232526; --accent-color: #ffb347; --bg-color: #f8f9fa; --text-color: #333; --light-text: #666; --card-bg: #fff; --code-bg: #f1f1f1; }
  body {
    font-family: 'Segoe UI', Arial, sans-serif;
    margin: 0;
    background: var(--bg-color);
    color: var(--text-color);
    line-height: 1.8;
  }
  
  .back-btn {
    display: inline-block;
    padding: 8px 16px;
    color: var(--primary-color);
    text-decoration: none;
    margin-bottom: 32px;
    transition: color 0.2s;
    font-weight: 500;
    border-radius: 4px;
    background: rgba(255, 179, 71, 0.1);
  }
  
  .back-btn:hover {
    color: var(--accent-color);
    background: rgba(255, 179, 71, 0.2);
  }
  
  .blog-post {
    max-width: 800px;
    margin: 0 auto;
  }
  
  .blog-meta {
    color: var(--light-text);
    margin: 16px 0 32px;
    font-size: 0.95em;
  }
  
  .blog-content {
    line-height: 1.8;
    font-size: 1.1em;
  }
  
  .blog-content h2 {
    margin-top: 48px;
    color: var(--primary-color);
    border-bottom: 2px solid var(--accent-color);
    padding-bottom: 8px;
    display: inline-block;
  }
  
  .blog-content h3 {
    margin-top: 32px;
    color: var(--primary-color);
  }
  
  .blog-content p {
    margin-bottom: 24px;
  }
  
  .blog-content img {
    max-width: 100%;
    border-radius: 8px;
    margin: 24px 0;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
  }
  
  .code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    overflow-x: auto;
    margin: 24px 0;
    font-family: 'Consolas', 'Monaco', monospace;
    font-size: 0.9em;
  }
  
  .visualization-container {
    margin: 40px 0;
    padding: 24px;
    background: var(--card-bg);
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
  }
  
  .visualization-controls {
    display: flex;
    gap: 16px;
    margin-bottom: 24px;
    flex-wrap: wrap;
  }
  
  .control-group {
    display: flex;
    flex-direction: column;
    gap: 8px;
  }
  
  button, select {
    padding: 8px 16px;
    background: var(--primary-color);
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background 0.2s;
  }
  
  button:hover {
    background: #414345;
  }
  
  .highlight {
    background: rgba(255, 179, 71, 0.2);
    padding: 2px 4px;
    border-radius: 4px;
  }
  
  .math-formula {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 8px;
    margin: 24px 0;
    overflow-x: auto;
    text-align: center;
  }
  
  .tabs {
    display: flex;
    gap: 4px;
    margin-bottom: 24px;
    border-bottom: 2px solid #eee;
    padding-bottom: 2px;
}

.tab {
    padding: 8px 16px;
    background: #f8f9fa;
    border-radius: 4px 4px 0 0;
    cursor: pointer;
    transition: all 0.2s;
    border: 1px solid #eee;
    border-bottom: none;
}

.tab:hover {
    background: #e9ecef;
}

.tab.active {
    background: var(--primary-color);
    color: white;
    border-color: var(--primary-color);
}

.tab-content {
    display: none;
    padding: 20px;
    background: #fff;
    border-radius: 4px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.05);
}

.tab-content.active {
    display: block;
    animation: fadeIn 0.3s ease-in;
}

.notebook-reference {
    background: #f8f9fa;
    border-left: 4px solid #0366d6;
    padding: 16px;
    margin: 24px 0;
    border-radius: 4px;
}

.implementation-note {
    background: #f0f7ff;
    border-left: 4px solid #79b8ff;
    padding: 12px 16px;
    margin: 16px 0;
    border-radius: 4px;
    font-size: 0.95em;
}

.implementation-note h3 {
    margin-top: 0;
    color: #0366d6;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.code-block {
    background: var(--code-bg);
    padding: 16px;
    border-radius: 4px;
    margin: 16px 0;
    font-family: 'Consolas', monospace;
}

.math-formula {
    padding: 24px;
    background: var(--code-bg);
    border-radius: 4px;
    margin: 16px 0;
    overflow-x: auto;
    text-align: center;
}
  
  .progress-container {
    height: 20px;
    width: 100%;
    background-color: #e9ecef;
    border-radius: 4px;
    margin: 16px 0;
  }
  
  .progress-bar {
    height: 100%;
    width: 0;
    background-color: var(--accent-color);
    border-radius: 4px;
    transition: width 0.3s ease;
  }
  
  .tooltip {
    position: absolute;
    padding: 8px;
    background: rgba(0,0,0,0.8);
    color: white;
    border-radius: 4px;
    pointer-events: none;
    font-size: 0.9em;
    z-index: 10;
  }
  
  @media (max-width: 768px) {
    #main {
      padding: 24px 5vw !important;
    }
    
    .visualization-controls {
      flex-direction: column;
    }
  }

  .comparison-container {
    margin: 32px 0;
    background: var(--card-bg);
    padding: 24px;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.08);
}

.comparison-table {
    width: 100%;
    border-collapse: separate;
    border-spacing: 0;
    margin-top: 20px;
    font-size: 0.95em;
}

.comparison-table th,
.comparison-table td {
    padding: 16px;
    text-align: left;
    border: 1px solid #eee;
}

.comparison-table th {
    background: var(--primary-color);
    color: white;
    font-weight: 500;
    position: relative;
}

.comparison-table th:first-child {
    border-top-left-radius: 8px;
}

.comparison-table th:last-child {
    border-top-right-radius: 8px;
}

.comparison-table tr:last-child td:first-child {
    border-bottom-left-radius: 8px;
}

.comparison-table tr:last-child td:last-child {
    border-bottom-right-radius: 8px;
}

.comparison-table td {
    background: white;
    transition: background-color 0.2s;
}

.comparison-table tr:nth-child(even) td {
    background: #f8f9fa;
}

.comparison-table tr:hover td {
    background: #f0f0f0;
}

.comparison-table td:first-child {
    font-weight: 500;
    color: var(--primary-color);
}

/* Add responsive styles */
@media (max-width: 768px) {
    .comparison-table {
        font-size: 0.85em;
    }
    
    .comparison-table th,
    .comparison-table td {
        padding: 12px;
    }
}

.comparison-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 20px;
    margin: 24px 0;
}

.grid-item {
    text-align: center;
}

.architecture-img {
    max-width: 100%;
    border-radius: 8px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
}

.img-caption {
    margin-top: 12px;
    color: var(--light-text);
    font-size: 0.9em;
}

@media (max-width: 768px) {
    .comparison-grid {
        grid-template-columns: 1fr;
    }
}

.reference-list {
    list-style-type: none;
    padding-left: 0;
}

.reference-list li {
    margin-bottom: 12px;
    line-height: 1.6;
}

.reference-list ul {
    padding-left: 20px;
    margin-top: 8px;
}

#references h3 {
    margin-top: 24px;
    color: var(--primary-color);
    font-size: 1.2em;
}

.example-details {
    background: #f8f9fa;
    padding: 16px;
    border-radius: 8px;
    margin: 16px 0;
}

.calculation-steps {
    background: #fff;
    padding: 20px;
    border-left: 4px solid var(--accent-color);
    margin: 16px 0;
}

.code-block {
    background: var(--code-bg);
    padding: 12px;
    border-radius: 4px;
    font-family: 'Consolas', monospace;
    font-size: 0.9em;
    margin: 8px 0;
}

.calculation-steps pre {
    background: #f8f9fa;
    padding: 12px;
    border-radius: 4px;
    margin: 8px 0;
}

.calculation-steps h4 {
    color: var(--primary-color);
    margin-top: 24px;
}

.key-observations {
    background: #fff3e0;
    padding: 16px;
    border-left: 4px solid #ffb347;
    margin-top: 24px;
    border-radius: 4px;
}

.example-details {
    background: #f8f9fa;
    padding: 20px;
    border-radius: 8px;
    margin: 24px 0;
}

.animation-container {
    background: white;
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    margin: 20px 0;
}

.animation-controls {
    text-align: center;
    margin-top: 10px;
}

.animation-controls button {
    margin: 0 5px;
    padding: 8px 16px;
    border: none;
    border-radius: 4px;
    background: var(--primary-color);
    color: white;
    cursor: pointer;
}

.animation-controls button:hover {
    background: var(--accent-color);
}

canvas {
    width: 100%;
    height: auto;
    margin: 10px 0;
}

.video-container {
    position: relative;
    padding-bottom: 56.25%; /* 16:9 aspect ratio */
    height: 0;
    overflow: hidden;
    margin: 24px 0;
    border-radius: 12px;
    box-shadow: 0 4px 16px rgba(0,0,0,0.1);
}

.video-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    border-radius: 12px;
}
  </style> </head> <body> <div id="main" style="padding: 32px 15vw;"> <a href="index.html" class="back-btn">&larr; Back to Home</a>
  <article class="blog-post">
    <h1>Everything about Autoencoders: From Linear Algebra to its Multimodal and Cross-modal Generative Capabilities</h1>
    <div class="blog-meta">May 5, 2025 -  Multimodal Unsupervised Learning -  By Soumya Savarn</div>
    
    <div class="blog-content">

      <!-- Motivation -->
      <section id="motivation">
        <h2>Motivation</h2>
        <p>My journey into generative models began with what seemed like an overwhelming field. However, my academic coursework in probability theory, Deep Learning, and Multimodal Data Processing provided the perfect foundation. Through this cumulative knowledge, I discovered that autoencoders offered an accessible entry point into understanding and working with Generative Multimodal models. I tried to build every concept from scratch, and you can watch these videos for deeper insights into my research journey:</p>
        
        <div class="video-container">
          <iframe 
            width="100%" 
            height="400" 
            src="https://www.youtube.com/embed/ooex1U_E1lM?si=fLTPfQlrYNS03gUt" 
            title="Understanding Autoencoders" 
            frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
            allowfullscreen>
          </iframe>
        </div>

        <p>These videos combine all my learning into a comprehensive overview. Thanks to my Multimodal Data Processing course project for rekindling my passion for sharing knowledge online!</p>
      </section>

      <!-- Historical Perspective -->
      <section id="history">
        <h2>Historical Perspective</h2>
        <p>Autoencoders trace back to early neural network research in the 1980s. Variants like denoising and variational autoencoders emerged after 2010, enabling applications in image and multimodal learning. Recent advances in cross-modal generation build on these foundations.</p>
      </section>

      <h2>Introduction</h2>
      <p>Autoencoders are a type of artificial neural network used for unsupervised learning. Their primary goal is to learn efficient representations of input data, typically for dimensionality reduction or noise removal. The architecture consists of an <b>encoder</b> that compresses the input, a <b>bottleneck</b> that stores the compressed knowledge, and a <b>decoder</b> that reconstructs the original data from this compressed form. 
      </p>
      <div class="implementation-note"> All code implementations discussed in this blog can be found in this <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572" target="_blank">Kaggle notebook</a>.
      </div>
      
      <div class="visualization-container">
        <img src="images/diagram-auto.png" alt="General Autoencoder Architecture" class="architecture-img">
        <p class="img-caption">Basic architecture of an autoencoder showing encoder, bottleneck, and decoder components</p>
      </div>

      <p>In this blog, we'll explore autoencoders from their mathematical foundations to advanced applications in multimodal learning. We'll also implement interactive visualizations to help you understand how autoencoders work.</p>

      <div class="animation-container">
        <canvas id="standardAutoencoder" width="800" height="300"></canvas>
        <div class="animation-controls">
            <button onclick="toggleAnimation('standard')">Play/Pause</button>
            <button onclick="resetAnimation('standard')">Reset</button>
        </div>
      </div>
      
      <h2>Autoencoder vs U-Net: Understanding the Differences</h2>
      <div class="comparison-container">
        <p>While both architectures use encoder-decoder structures, they serve different purposes and have key differences:</p>
        
        <table class="comparison-table">
          <tr>
            <th>Feature</th>
            <th>Autoencoder</th>
            <th>U-Net</th>
          </tr>
          <tr>
            <td>Primary Purpose</td>
            <td>Unsupervised learning, dimensionality reduction, feature learning</td>
            <td>Supervised segmentation, dense prediction tasks</td>
          </tr>
          <tr>
            <td>Architecture</td>
            <td>Symmetric encoder-decoder with bottleneck</td>
            <td>Encoder-decoder with skip connections between corresponding layers</td>
          </tr>
          <tr>
            <td>Information Flow</td>
            <td>All information passes through bottleneck</td>
            <td>Features can bypass bottleneck through skip connections</td>
          </tr>
          <tr>
            <td>Output Size</td>
            <td>Same as input (reconstruction)</td>
            <td>Can be different from input (segmentation map)</td>
          </tr>
          <tr>
            <td>Training</td>
            <td>Self-supervised (input = target)</td>
            <td>Supervised (requires labeled data)</td>
          </tr>
        </table>
      </div>
      
      
      
      <h2>Mathematical Foundation</h2>
      <p>An autoencoder is defined by two main components: an <span class="highlight">encoder function</span> that transforms the input data, and a <span class="highlight">decoder function</span> that recreates the input data from the encoded representation.</p>
      
      <p>Formally, for input space X and encoded space Z, we define:</p>
      <div class="math-formula">
        \[ \text{Encoder}: f_\phi: \mathcal{X} \rightarrow \mathcal{Z} \]
        \[ \text{Decoder}: g_\theta: \mathcal{Z} \rightarrow \mathcal{X} \]
      </div>
      
      <p>The training objective is to minimize the reconstruction error:</p>
      <div class="math-formula">
        \[ \mathcal{L}(\phi, \theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\phi(\mathbf{x}_i))\|^2 \]
      </div>
      
      <p>Surprisingly, for linear autoencoders (using only linear activations), the optimal solution is equivalent to Principal Component Analysis (PCA). This mathematical connection is well-explained in <a href="https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/src/Lecture7/modules/Module2/Lecture7_2.pdf" target="_blank">these lecture notes from IIT Madras's Deep Learning course</a> and rigorously proven in <a href="https://arxiv.org/abs/1804.10253" target="_blank">this paper by Plaut (2018)</a>. The linear autoencoder learns to project data onto the principal subspace, just like PCA.</p>
      <div class="implementation-note">
        <p>üí° For practical implementation of linear autoencoders and their equivalence to PCA, check <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572#Appendix-1:-Linear-Autoencoders-and-PCA" target="_blank">Appendix 1 of the  notebook</a>.</p>
      </div>
      
      <div class="visualization-container comparison-grid">
        <div class="grid-item">
            <img src="images/linear_auto.png" alt="Linear Autoencoder Architecture" class="architecture-img">
            <p class="img-caption">Linear Autoencoder Structure</p>
        </div>
        <div class="grid-item">
            <img src="images/pca.png" alt="PCA Visualization" class="architecture-img">
            <p class="img-caption">Principal Component Analysis (PCA)</p>
        </div>
      </div>

      <div class="math-formula">
        \[ \mathcal{L}(\phi, \theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\phi(\mathbf{x}_i))\|^2 \]
        <p class="formula-caption">Linear autoencoder loss function, which under optimal conditions yields PCA solution</p>
      </div>
      
      <div class="visualization-container">
        <h2><strong>BONUS: </strong>Recirculation Algorithm</h2>
        <h4>You may skip this part.</h4>
  <p>
    It is very rare to find something related to it in mainstream machine learning community.
    The Recirculation Algorithm is an alternative to backpropagation for training neural networks, particularly autoencoders. It was introduced by Geoffrey Hinton and James McClelland in 1987 as a more biologically plausible learning mechanism. 
    (<a href="https://www.mdpi.com/2227-7390/11/8/1777?utm_source=chatgpt.com" target="_blank">MDPI</a>)
  </p>

  <h3> Key Reference</h3>
  <ul>
    <li>
      <strong>Hinton, G.E., & McClelland, J.L. (1987).</strong> 
      <em>Learning Representations by Recirculation</em>. 
      In <em>Proceedings of the Neural Information Processing Systems</em>, Denver, CO, USA. MIT Press: Cambridge, MA, USA.
      <br>
      <a href="https://www.mdpi.com/2227-7390/11/8/1777" target="_blank">Link to reference</a>
    </li>
  </ul>

  <p>
    In this foundational paper, Hinton and McClelland propose the recirculation algorithm as a method for training neural networks without the need for explicit error backpropagation. Instead, the network adjusts its weights based on the difference between the input and the reconstructed output, allowing for local learning rules that are more aligned with biological neural processes.
  </p>

  <h3> Overview of the Recirculation Algorithm</h3>
  <p>The recirculation algorithm operates in two main phases:</p>
  <ol>
    <li><strong>Forward Phase</strong>: The input data is passed through the network to produce an output.</li>
    <li><strong>Backward (Recirculation) Phase</strong>: The output is then fed back into the network as input, and the network processes this "recirculated" data to produce a reconstruction.</li>
  </ol>

  <p>
    The weights are updated based on the difference between the original input and the reconstruction, using local learning rules. This approach allows the network to learn representations by minimizing the discrepancy between the input and its reconstruction without relying on global error signals.
    (<a href="https://pubmed.ncbi.nlm.nih.gov/30317133/?utm_source=chatgpt.com" target="_blank">PubMed</a>)
  </p>

  <h3> Further Reading</h3>
  <ul>
    <li>
      <strong>Buscema, P.M.</strong> 
      <em>Recirculation Neural Networks</em>. 
      <a href="https://www.academia.edu/5743527/Recirculation_Neural_Networks" target="_blank">Link to paper</a>
    </li>
    <li>
      <strong>Baldi, P., & Sadowski, P. (2018).</strong> 
      <em>Learning in the Machine: Recirculation is Random Backpropagation</em>. 
      <a href="https://pubmed.ncbi.nlm.nih.gov/30317133/" target="_blank">Link to article</a>
    </li>
  </ul>

    <p>
      These works delve into the theoretical underpinnings of the recirculation algorithm and its relationship to other learning methods, offering valuable insights into alternative approaches to training neural networks.
    </p>

       
        <div class="math-formula">
          <h3>Forward Pass:</h3>
          \[ h = \sigma(W_1 x) \]
          \[ y = \sigma(W_2 h) \]
          <p class="formula-caption">where \(\sigma\) is the sigmoid activation function</p>
        </div>

        <div class="math-formula">
          <h3>Recirculation:</h3>
          \[ h_{rec} = \sigma(W_2^T y) \]
          <p class="formula-caption">where \(W_2^T\) is the transpose of the decoder weights</p>
          The above formula holds true only when we are enforcing \[W_2^T = W_1\].
        </div>

        <div class="math-formula">
          <h3>Weight Update:</h3>
          \[ \Delta W_1 = \eta (h_{rec} - h) x^T \]
          <p class="formula-caption">where \(\eta\) is the learning rate</p>
        </div>

        <h3>Key Properties:</h3>
        <ul>
          <li><strong>Local Learning:</strong> Updates only use information available at each synapse</li>
          <li><strong>Biological Plausibility:</strong> No separate error backpropagation channel needed</li>
          <li><strong>Hardware Efficiency:</strong> Well-suited for neuromorphic computing systems</li>
        </ul>

        <div class="implementation-note">
          <h3>Worked Example: 2‚Üí1‚Üí2 Linear Recirculation Autoencoder</h3>
          <p>Let's see how the recirculation algorithm works in practice with a simple example:</p>

          <div class="example-details">
            <ul>
              <li><strong>Architecture:</strong> 2 input units ‚Üí 1 hidden unit ‚Üí 2 output units</li>
              <li><strong>Activation:</strong> Linear (no nonlinearity)</li>
              <li><strong>Learning rate:</strong> 0.1</li>
              <li><strong>Initial Weights:</strong></li>
            </ul>
            <pre class="code-block">
W1 = [[ 0.1], [-0.1]]  # 2√ó1 matrix
W2 = [[ 0.2, -0.3]]    # 1√ó2 matrix</pre>
          </div>

          <div class="calculation-steps">
            <h4>Training Example: x = [1, 0]</h4>
            <ol>
              <li>Forward pass:
                <ul>
                  <li>h = W1·µÄx = 0.1</li>
                  <li>y = W2h = [0.02, -0.03]</li>
                </ul>
              </li>
              <li>Recirculation:
                <ul>
                  <li>hrec = W1·µÄy = 0.005</li>
                  <li>yrec = W2hrec = [0.001, -0.0015]</li>
                </ul>
              </li>
              <li>Weight updates:
                <ul>
                  <li>ŒîW1 = Œ∑(hrec - h)x = 0.1(0.005 - 0.1)[1, 0]·µÄ</li>
                  <li>Updated W1 = [[0.109995], [-0.0999925]]</li>
                </ul>
              </li>
            </ol>
          </div>

          <div class="math-formula">
            <p>The process continues iteratively, gradually improving reconstruction quality.</p>
          </div>
        </div>

        <div class="example-details">
          <h3>Detailed Step-by-Step Calculation</h3>
          
          <h4>Initial Setup:</h4>
          <pre class="code-block">
Input x = [1, 0]   # 2D input vector
W1 = [[ 0.1],      # 2√ó1 encoder matrix
      [-0.1]]      
W2 = [[ 0.2, -0.3]]# 1√ó2 decoder matrix
Learning rate Œ∑ = 0.1</pre>
        
          <div class="calculation-steps">
            <h4>1. Forward Pass</h4>
            <ol>
              <li>Calculate hidden representation h:
                <pre class="code-block">
h = W1·µÄx
h = [0.1, -0.1] ¬∑ [1, 0]
h = 0.1 ¬∑ 1 + (-0.1) ¬∑ 0
h = 0.1</pre>
              </li>
              <li>Calculate output y:
                <pre class="code-block">
y = W2h
y = [[0.2, -0.3]] ¬∑ [0.1]
y = [0.2 ¬∑ 0.1, -0.3 ¬∑ 0.1]
y = [0.02, -0.03]</pre>
              </li>
            </ol>
        
            <h4>2. Recirculation Phase</h4>
            <ol>
              <li>Calculate recirculated hidden state hrec:
                <pre class="code-block">
hrec = W1·µÄy
hrec = [0.1, -0.1] ¬∑ [0.02, -0.03]
hrec = 0.1 ¬∑ 0.02 + (-0.1) ¬∑ (-0.03)
hrec = 0.002 + 0.003
hrec = 0.005</pre>
              </li>
              <li>Calculate recirculated output yrec:
                <pre class="code-block">
yrec = W2hrec
yrec = [[0.2, -0.3]] ¬∑ [0.005]
yrec = [0.2 ¬∑ 0.005, -0.3 ¬∑ 0.005]
yrec = [0.001, -0.0015]</pre>
              </li>
            </ol>
        
            <h4>3. Weight Updates</h4>
            <ol>
              <li>Calculate error term:
                <pre class="code-block">
error = hrec - h
error = 0.005 - 0.1
error = -0.095</pre>
              </li>
              <li>Calculate weight updates:
                <pre class="code-block">
ŒîW1 = Œ∑(hrec - h)x·µÄ
ŒîW1 = 0.1 ¬∑ (-0.095) ¬∑ [1, 0]
ŒîW1 = [[-0.0095],
       [0]]</pre>
              </li>
              <li>Update weights:
                <pre class="code-block">
W1_new = W1 + ŒîW1
W1_new = [[0.1],    + [[-0.0095],
          [-0.1]]     [0]]
W1_new = [[0.109995],
          [-0.0999925]]</pre>
              </li>
            </ol>
          </div>
        
          <div class="key-observations">
            <h4>Key Observations:</h4>
            <ul>
              <li>The recirculation error (-0.095) indicates how far the recirculated hidden state is from the original</li>
              <li>Only the first row of W1 gets updated significantly because x = [1, 0]</li>
              <li>The weight update moves W1 in a direction that would make hrec closer to h in the next iteration</li>
            </ul>
          </div>
        </div>
      
    </div>



      <h2>Types of Autoencoders</h2>
      <p>There are several variations of autoencoders, each with specific applications:</p>
      
      <div class="tabs">
        <div class="tab active" data-tab="vanilla">Vanilla</div>
        <div class="tab" data-tab="sparse">Sparse</div>
        <div class="tab" data-tab="denoising">Denoising</div>
        <div class="tab" data-tab="variational">Variational (VAE)</div>
        <div class="tab" data-tab="convolutional">Convolutional</div>
      </div>
      
      <div class="tab-content active" id="vanilla-content">
        <h3>Vanilla Autoencoder</h3>
        <p>The basic autoencoder architecture consists of an encoder and decoder with fully connected layers. The encoder compresses the input to a lower-dimensional code, and the decoder reconstructs the input from this code.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\mathbf{x}_i))\|^2 \]
          <p class="formula-caption">where \(f_\theta\) is the encoder and \(g_\theta\) is the decoder</p>
        </div>

      </div>
      
      <div class="tab-content" id="sparse-content">
        <h3>Sparse Autoencoder</h3>
        <p>Sparse autoencoders add a sparsity constraint to the hidden layer, forcing the model to activate only a small number of neurons at a time. This encourages the model to learn more robust features.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\mathbf{x}_i))\|^2 + \lambda \sum_{j=1}^m |\rho - \hat{\rho_j}| \]
          <p class="formula-caption">where \(\rho\) is the target sparsity and \(\hat{\rho_j}\) is the average activation of hidden unit j</p>
        </div>
      </div>
      
      <div class="tab-content" id="denoising-content">
        <h3>Denoising Autoencoder</h3>
        <p>Denoising autoencoders are trained to reconstruct clean inputs from corrupted versions. This makes them robust to noise and helps them learn more useful features.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N \|\mathbf{x}_i - g_\theta(f_\theta(\tilde{\mathbf{x}}_i))\|^2 \]
          <p class="formula-caption">where \(\tilde{\mathbf{x}}_i = \mathbf{x}_i + \epsilon\) is the corrupted input with noise \(\epsilon\)</p>
        </div>
      </div>
      
      <div class="tab-content" id="variational-content">
        <h3>Variational Autoencoder (VAE)</h3>
        <p>Variational autoencoders are probabilistic models that learn a latent variable model for the input data. Instead of encoding an input as a single point, they encode it as a distribution over the latent space.</p>
        <p>The VAE loss function has two components: the reconstruction loss and the KL divergence between the encoder's distribution and a prior distribution.</p>
        <div class="math-formula">
          \[ \mathcal{L}(\phi, \theta, \mathbf{x}) = -\mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})] + D_{\text{KL}}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) \]
        </div>

      </div>
      
      <div class="tab-content" id="convolutional-content">
        <h3>Convolutional Autoencoder</h3>
        <p>Convolutional autoencoders use convolutional layers in both the encoder and decoder, making them well-suited for image data.</p>
        <div class="math-formula">
          \[ f_\theta(\mathbf{x}) = \sigma(\text{Conv2D}(\mathbf{x}) \ast \mathbf{W} + \mathbf{b}) \]
          \[ g_\theta(\mathbf{z}) = \sigma(\text{Conv2DTranspose}(\mathbf{z}) \ast \mathbf{W}' + \mathbf{b}') \]
          <p class="formula-caption">where \(W, W'\) are learnable filters and \(\sigma\) is an activation function</p>
        </div>
      </div>

      <p>For a comprehensive theoretical background on Autoencoders and Variational Autoencoders (VAEs), I highly recommend Chapter 14 and 20 of <a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a> by Goodfellow, Bengio, and Courville.</p>

      <h2>Applications</h2>
      <p>Autoencoders serve multiple purposes in machine learning and data science. They excel at dimensionality reduction by compressing high-dimensional data while preserving key features. Their ability to learn normal data patterns makes them effective for anomaly detection through reconstruction error analysis. In image processing, denoising autoencoders can clean corrupted images. The encoded representations provide valuable features for downstream tasks like classification. Additionally, variants like variational autoencoders enable generative modeling, creating new data samples that mirror the training distribution.</p>

      <p>Let's explore <strong>Variational Autoencoders</strong> (VAEs) and their fascinating applications in multimodal learning. For a rigorous mathematical treatment and detailed proofs, I highly recommend referring to <a href="https://arxiv.org/pdf/1606.05908" target="_blank">this seminal paper</a> on multimodal VAEs.</p>

      <div class="animation-container">
        <canvas id="vaeAutoencoder" width="800" height="300"></canvas>
        <div class="animation-controls">
            <button onclick="toggleAnimation('vae')">Play/Pause</button>
            <button onclick="resetAnimation('vae')">Reset</button>
        </div>
      </div>

       <!-- Convex Interpolation in VAEs -->
<section id="convex-interpolation">
  <h2>Convex Interpolation in Variational Autoencoders</h2>
  <p>
    I‚Äôve found that (obviously the researchers) in a VAE, because we force the latent variable \(z\) to follow a simple prior like \(p(z)=\mathcal{N}(0,I)\), the resulting latent space becomes (approximately) convex‚Äîany linear mix 
    \[
      \mathbf{z}_\alpha = (1-\alpha)\mathbf{z}_1 + \alpha\mathbf{z}_2,\quad \alpha\in[0,1]
    \]
    stays in a high‚Äìdensity region where the decoder can produce meaningful outputs .
  </p>
  <p>
    Concretely, decoding \(\mathbf{z}_\alpha\) via \(\mathbf{x}_\alpha = g_\theta(\mathbf{z}_\alpha)\) often yields smooth ‚Äúin‚Äëbetween‚Äù samples that semantically blend the endpoints .  
  </p>
  <p>
    However, naive latent interpolations can stray off the true data manifold, leading to artifacts. Recent work shapes the latent manifold to be locally convex‚Äîe.g., by adding a regularizer that encourages
    \[
      \|g_\theta((1-\alpha)\mathbf{z}_i + \alpha \mathbf{z}_j) - ((1-\alpha)\mathbf{x}_i + \alpha \mathbf{x}_j)\|\;\text{to be small}
    \]
    for many \(\alpha\) .  
  </p>
  <p>
    Statisticians call evaluation inside the convex hull of training points ‚Äúinterpolation,‚Äù which is generally safer than extrapolating outside that hull .
  </p>
  <section id="thought-experiment" class="implementation-note">
    <h3>Food for Thought: Voice Interpolation</h3>
    <p>
      Think about this‚Äîwhat if we could map the voices of two famous singers, like Arijit Singh and Atif Aslam, and then find a smooth path from one voice to the other? Kind of like moving a slider from one style to the next. That would be amazing, right?
    </p>
    
    <p>
      Now take it a step further‚Äîwhat if you could see how close your voice is to your favorite singer's? Just like with face filters where you can see how your face compares to someone else's, we could do something similar with voice. You could actually learn how your voice is different, and maybe even change it to sound more like theirs.
    </p>
    
    <p>
      The idea sounds fun‚Äîand the possibilities are endless!
    </p>
  </section>
  

  <div class="visualization-container">
    <img src="images/convex.png" alt="Convex Interpolation in Latent Space" class="architecture-img">
    <p class="img-caption">Visualization of convex interpolation in the VAE latent space</p>
  </div>
  <div class="implementation-note">
    <p>üí° To see convex interpolation in action with implementation details, visit <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572#Appendix-2:-Convex-Interpolation" target="_blank">Appendix 2 of the Kaggle notebook</a>.</p>
  </div>
</section>

<!-- Insights from ‚ÄúMultimodal Generative Models for Scalable Weakly‚ÄëSupervised Learning‚Äù -->
<section id="mvae-motivation">
  <h2>Moving to multimodal generation</h2>
  <p>
    By now you must have a good clue that the VAE is a generative model.
    It can generate new data points by sampling from the latent space and decoding them back to the original space. 
    This property is particularly useful in multimodal learning, where we want 
    to generate data that combines information from multiple modalities.
    A naive way to do this is to concatenate the latent variables from each modality and train a single VAE on the combined data.
    However, this approach has several limitations:
  </p>
  <ul>
    <li>It requires a large amount of labeled data for each modality, which is often not available.</li>
    <li>It does not leverage the relationships between modalities, leading to suboptimal representations.</li>
    <li>It can be computationally expensive and slow to train.</li>
  </ul>
</section>

<!-- Insights from ‚ÄúMultimodal Generative Models for Scalable Weakly‚ÄëSupervised Learning‚Äù -->
<div>
  <h2>Insights from "Multimodal Generative Models for Scalable Weakly‚ÄëSupervised Learning"</h2>

  <p>I first encountered the MVAE paper by Mike Wu and Noah Goodman (<a href="https://arxiv.org/pdf/1802.05335" target="_blank">Wu & Goodman, 2018</a>) when I was looking for a way to merge different data types‚Äîlike images, text, and audio‚Äîinto one shared hidden representation called <em>z</em>. In their approach, each modality's encoder outputs a Gaussian "expert" over <em>z</em>, and they multiply those experts together along with a simple Gaussian prior. This trick, known as the product‚Äëof‚Äëexperts (PoE), means that as I feed in more modalities, the combined Gaussian becomes tighter (its variance shrinks), so I become more confident about the value of <em>z</em> when I have richer information.</p>

  <p>Behind the scenes, training the MVAE means maximizing a modified evidence lower bound (ELBO). On one hand, I want to reconstruct each modality from <em>z</em>, which pushes up the sum of log‚Äëlikelihoods for every modality. On the other hand, I add a KL divergence term that keeps my fused posterior close to the standard normal prior, preventing the model from becoming overconfident. To handle missing data, the authors alternate between fully observed examples‚Äîwhere all encoders and the PoE get updated‚Äîand partially observed examples‚Äîwhere only the single‚Äëmodality encoder for the available data is updated. This sub‚Äësampling trick makes sure I can still infer a sensible <em>z</em> even if some streams drop out.</p>

  <p>An elegant aspect of MVAE is its shared decoder. Instead of building one large decoder per modality, they attach small ‚Äúheads‚Äù for each output type to a single backbone network. This reduces the total number of parameters and forces the model to learn a core latent space that works across all data types.</p>


  <style>
    code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    .example { background-color: #eef; padding: 10px; border-left: 4px solid #88f; margin: 10px 0; }
  </style>
<div class="visualization-container">
  <img src="images/poes.png" alt="Product of Experts Architecture" class="architecture-img">
  <p class="img-caption">Product of Experts (PoE) combining multiple modality encoders</p>
</div>

  <p>A Variational Autoencoder (VAE) models data <em>x</em> using a latent variable <em>z</em>, with a prior <code>p(z)</code> (often a standard Gaussian) and a decoder <code>p<sub>Œ∏</sub>(x|z)</code> implemented by a neural network. Because the true data likelihood is hard to compute, we optimize the <strong>Evidence Lower Bound (ELBO)</strong> instead:</p>
  <pre><code>ELBO(x) = E<sub>q<sub>œÜ</sub>(z|x)</sub>[log p<sub>Œ∏</sub>(x|z)] - KL(q<sub>œÜ</sub>(z|x) || p(z))</code></pre>
  <p>Here, <code>q<sub>œÜ</sub>(z|x)</code> is an encoder (inference network) approximating the posterior, and <code>KL</code> is the Kullback-Leibler divergence.</p>

  <h3>Extending to Multiple Modalities</h3>
  <p>If we have <em>N</em> different inputs (modalities) <code>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub></code> (e.g., image, text, audio), we assume they are independent given <code>z</code>. The generative model becomes:</p>
  <pre><code>p(z, x<sub>1</sub>, ..., x<sub>N</sub>) = p(z) &times; ‚àè p<sub>Œ∏</sub>(x<sub>i</sub>|z)</code></pre>
  <p>When some modalities are missing, we simply ignore them in training.</p>

  <h3>Combining Encoders with Product-of-Experts</h3>
  <p>We need a combined encoder <code>q(z|x<sub>1</sub>,...,x<sub>N</sub>)</code>. A neat trick is the <em>Product-of-Experts (PoE)</em>:</p>
  <pre><code>q(z|x<sub>1</sub>,...,x<sub>N</sub>) ‚àù p(z) &times; ‚àè qÃÉ(z|x<sub>i</sub>),</code></pre>
  <p>where each <code>qÃÉ(z|x<sub>i</sub>)</code> is a Gaussian from the encoder for modality <em>i</em>, and <code>p(z)</code> acts as a "prior expert."</p>

  <h3>Numerical Example</h3>
  <div class="example">
    <p>Suppose we have <strong>2 modalities</strong> with Gaussian encoders:</p>
    <ul>
      <li>Modality 1: <code>qÃÉ(z|x<sub>1</sub>) = N(z; Œº<sub>1</sub>=2, œÉ<sub>1</sub><sup>2</sup>=1)</code></li>
      <li>Modality 2: <code>qÃÉ(z|x<sub>2</sub>) = N(z; Œº<sub>2</sub>=0, œÉ<sub>2</sub><sup>2</sup>=4)</code></li>
      <li>Prior: <code>p(z) = N(z; 0, 1)</code></li>
    </ul>
    <p>Compute precision (<code>T = 1/variance</code>):</p>
    <ul>
      <li>T<sub>1</sub> = 1/1 = 1</li>
      <li>T<sub>2</sub> = 1/4 = 0.25</li>
      <li>T<sub>0</sub> (prior) = 1/1 = 1</li>
    </ul>
    <p><strong>Combined precision:</strong> T = T<sub>0</sub> + T<sub>1</sub> + T<sub>2</sub> = 1 + 1 + 0.25 = 2.25</p>
    <p><strong>Combined mean:</strong></p>
    <pre><code>Œº = (T<sub>0</sub>¬∑0 + T<sub>1</sub>¬∑2 + T<sub>2</sub>¬∑0) / T = (0 + 2 + 0) / 2.25 ‚âà 0.89</code></pre>
    <p><strong>Combined variance:</strong> œÉ<sup>2</sup> = 1 / T = 1 / 2.25 ‚âà 0.44</p>
    <p>So the multimodal posterior is <code>N(z; Œº‚âà0.89, œÉ‚âà0.66)</code>. This single Gaussian summarizes both inputs and the prior!</p>
  </div>

  <h3>Putting It All Together</h3>
  <p>1. <strong>Encode each modality</strong>: get its <code>Œº<sub>i</sub></code> and <code>œÉ<sub>i</sub></code>.<br>
  2. <strong>Combine with PoE</strong>: use the formulas above to get joint <code>Œº</code> and <code>œÉ</code>.<br>
  3. <strong>Reparameterize</strong>: sample <code>z = Œº + œÉ ‚äô Œµ</code> with Œµ‚àºN(0, I).<br>
  4. <strong>Decode</strong>: reconstruct each modality with <code>p<sub>Œ∏</sub>(x<sub>i</sub>|z)</code>.<br>
  5. <strong>Optimize ELBO</strong>: sum reconstruction losses and KL term.</p>

  <p>This simple approach lets us handle any combination of missing modalities without training separate encoders for each subset!</p>
  <div class="implementation-note">
    <p>üí° For a complete implementation of the Product of Experts (PoE) approach, check out <a href="https://www.kaggle.com/code/soumyasavarn/everything-about-autoencoders/notebook?scriptVersionId=238355572#Appendix-3:-Product-of-Experts" target="_blank">Appendix 3 of the  notebook</a>.</p>
  </div>

  
</div>

<section id="references">
  <h2>References</h2>
  
  <h3>Papers</h3>
  <ul class="reference-list">
    <li>Wu, M., & Goodman, N. (2018). <a href="https://arxiv.org/pdf/1802.05335" target="_blank">Multimodal Generative Models for Scalable Weakly-Supervised Learning</a>. arXiv preprint arXiv:1802.05335.</li>
    <li>Dai, B., et al. (2021). <a href="https://arxiv.org/abs/2110.04121" target="_blank">Multimodal Conditional Image Synthesis with Product-of-Experts GANs</a>. arXiv preprint arXiv:2110.04121.</li>
    <li>Plaut, E. (2018). <a href="https://arxiv.org/abs/1804.10253" target="_blank">From Principal Subspaces to Principal Components with Linear Autoencoders</a>. arXiv preprint arXiv:1804.10253.</li>
    <li>Hinton, G.E., & McClelland, J.L. (1987). Learning Representations by Recirculation. In <em>Neural Information Processing Systems</em>.</li>
  </ul>

  <h3>Books</h3>
  <ul class="reference-list">
    <li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <a href="https://www.deeplearningbook.org/" target="_blank">Deep Learning</a>. MIT Press.</li>
  </ul>

  <h3>Videos</h3>
  <ul class="reference-list">
    <li>Deepia. (2023). <a href="https://www.youtube.com/watch?v=qJeaCHQ1k2w" target="_blank">Understanding Autoencoders: A Visual Guide</a>.</li>
  </ul>

  <h3>Tools Used</h3>
  <ul class="reference-list">
    <li><strong>Code Assistance:</strong>
      <ul>
        <li>Claude 3.7 Sonnet - JavaScript interactivity and animations</li>
        <li>GitHub Copilot - Image placement and layout assistance</li>
        <li>Microsoft OneNote - For explanation in the video</li>
      </ul>
    </li>
    <li><strong>Deployment:</strong> Vercel</li>
  </ul>
</section>

      <script>
        // Initialize tab functionality
        document.addEventListener('DOMContentLoaded', () => {
            const tabs = document.querySelectorAll('.tab');
            const contents = document.querySelectorAll('.tab-content');
            
            // Set initial state
            contents.forEach(content => content.style.display = 'none');
            document.querySelector('#vanilla-content').style.display = 'block';
            document.querySelector('[data-tab="vanilla"]').classList.add('active');
            
            // Add click handlers to tabs
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    // Remove active class from all tabs
                    tabs.forEach(t => t.classList.remove('active'));
                    
                    // Hide all content sections
                    contents.forEach(content => content.style.display = 'none');
                    
                    // Add active class to clicked tab
                    tab.classList.add('active');
                    
                    // Show corresponding content
                    const contentId = `${tab.dataset.tab}-content`;
                    document.getElementById(contentId).style.display = 'block';
                });
            });
        });

        let animationFrames = {};
        let isAnimating = {standard: true, vae: true};
        let t = {standard: 0, vae: 0};

        const colors = {
            input: '#4CAF50',
            hidden: '#2196F3',
            output: '#FF5722',
            flow: '#9C27B0',
            distribution: '#FFC107'
        };

        function drawNode(ctx, x, y, color, label = '') {
            ctx.beginPath();
            ctx.arc(x, y, 15, 0, Math.PI * 2);
            ctx.fillStyle = color;
            ctx.fill();
            ctx.strokeStyle = '#333';
            ctx.stroke();
            
            if (label) {
                ctx.fillStyle = '#333';
                ctx.font = '14px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(label, x, y + 30);
            }
        }

        function drawFlow(ctx, x1, y1, x2, y2, t) {
            const gradient = ctx.createLinearGradient(x1, y1, x2, y2);
            const progress = (Math.sin(t * 2) + 1) / 2;
            
            gradient.addColorStop(0, 'rgba(156, 39, 176, 0.1)');
            gradient.addColorStop(progress, 'rgba(156, 39, 176, 0.8)');
            gradient.addColorStop(1, 'rgba(156, 39, 176, 0.1)');
            
            ctx.beginPath();
            ctx.moveTo(x1, y1);
            ctx.lineTo(x2, y2);
            ctx.strokeStyle = gradient;
            ctx.lineWidth = 2;
            ctx.stroke();
        }

        function drawStandardAutoencoder(ctx) {
            ctx.clearRect(0, 0, 800, 300);
            const inputX = 100;
            const hiddenX = 400;
            const outputX = 700;
            
            // Draw input nodes
            for (let i = 0; i < 4; i++) {
                const y = 100 + i * 50;
                drawNode(ctx, inputX, y, colors.input, `x${i+1}`);
                
                // Connections to hidden nodes
                drawFlow(ctx, inputX + 15, y, hiddenX - 15, 125, t.standard + i * 0.2);
                drawFlow(ctx, inputX + 15, y, hiddenX - 15, 175, t.standard + i * 0.2);
            }
            
            // Draw hidden nodes
            drawNode(ctx, hiddenX, 125, colors.hidden, 'z1');
            drawNode(ctx, hiddenX, 175, colors.hidden, 'z2');
            
            // Draw output nodes
            for (let i = 0; i < 4; i++) {
                const y = 100 + i * 50;
                drawNode(ctx, outputX, y, colors.output, `x'${i+1}`);
                
                // Connections from hidden nodes
                drawFlow(ctx, hiddenX + 15, 125, outputX - 15, y, t.standard + i * 0.2);
                drawFlow(ctx, hiddenX + 15, 175, outputX - 15, y, t.standard + i * 0.2);
            }
        }

        function drawVAE(ctx) {
            ctx.clearRect(0, 0, 800, 300);
            const inputX = 100;
            const encoderX = 300;
            const sampleX = 500;
            const outputX = 700;
            
            // Draw input nodes
            for (let i = 0; i < 4; i++) {
                const y = 100 + i * 50;
                drawNode(ctx, inputX, y, colors.input, `x${i+1}`);
                
                // Connections to encoder
                drawFlow(ctx, inputX + 15, y, encoderX - 15, 125, t.vae + i * 0.2);
                drawFlow(ctx, inputX + 15, y, encoderX - 15, 175, t.vae + i * 0.2);
            }
            
            // Draw encoder output (Œº, œÉ)
            drawNode(ctx, encoderX, 125, colors.hidden, 'Œº');
            drawNode(ctx, encoderX, 175, colors.hidden, 'œÉ');
            
            // Draw sampling node
            drawNode(ctx, sampleX, 150, colors.distribution, 'z');
            
            // Draw sampling process
            ctx.beginPath();
            ctx.moveTo(encoderX + 15, 150);
            ctx.bezierCurveTo(
                encoderX + 50, 150,
                sampleX - 50, 150 + Math.sin(t.vae * 3) * 20,
                sampleX - 15, 150
            );
            ctx.strokeStyle = colors.flow;
            ctx.stroke();
            
            // Draw output nodes
            for (let i = 0; i < 4; i++) {
                const y = 100 + i * 50;
                drawNode(ctx, outputX, y, colors.output, `x'${i+1}`);
                drawFlow(ctx, sampleX + 15, 150, outputX - 15, y, t.vae + i * 0.2);
            }
        }

        function animate() {
            if (isAnimating.standard) {
                t.standard += 0.05;
                const standardCtx = document.getElementById('standardAutoencoder').getContext('2d');
                drawStandardAutoencoder(standardCtx);
            }
            if (isAnimating.vae) {
                t.vae += 0.05;
                const vaeCtx = document.getElementById('vaeAutoencoder').getContext('2d');
                drawVAE(vaeCtx);
            }
            animationFrames.standard = requestAnimationFrame(animate);
        }

        function toggleAnimation(type) {
            isAnimating[type] = !isAnimating[type];
        }

        function resetAnimation(type) {
            t[type] = 0;
            const ctx = document.getElementById(`${type}Autoencoder`).getContext('2d');
            if (type === 'standard') {
                drawStandardAutoencoder(ctx);
            } else {
                drawVAE(ctx);
            }
        }

        // Initialize animations when document loads
        document.addEventListener('DOMContentLoaded', () => {
            animate();
        });
      </script>

     
    </div>
  </article>
  </div>  
  
  </body>
   </html>